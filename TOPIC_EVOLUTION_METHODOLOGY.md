# Topic Landscape Evolution - Calculation Methodology

## Overview

The Topic Landscape Evolution metric uses **unsupervised machine learning** (clustering) on paper embeddings to identify research topics and track how they evolve over time. This is a sophisticated approach that goes beyond simple keyword matching.

---

## Step-by-Step Calculation Process

### Step 1: Data Preparation (Lines 99-119)

For each 5-year interval (e.g., 1985-1989, 1990-1994, etc.):

1. **Get all papers** published in that interval from Neo4j
2. **Filter papers** that have embeddings (384-dimensional vectors generated by SentenceTransformer)
3. **Skip intervals** with fewer than 3 papers (insufficient for clustering)

```python
# Query Neo4j for papers with embeddings
MATCH (p:Paper)
WHERE p.paper_id IN $paper_ids
AND p.embedding IS NOT NULL
RETURN p.paper_id, p.embedding, p.title, p.abstract
```

---

### Step 2: Embedding Extraction (Lines 146-151)

- Extract the **384-dimensional embedding vectors** for each paper
- These embeddings capture semantic meaning of the paper (title + abstract)
- Model used: `all-MiniLM-L6-v2` (SentenceTransformer)

**What are embeddings?**
- Dense vector representations that capture semantic similarity
- Papers with similar topics will have similar embeddings
- Example: Papers about "mergers and acquisitions" will cluster together

---

### Step 3: Optimal Cluster Count Determination (Lines 153-155)

**Problem**: How many topics exist in this interval?

**Solution**: Simplified elbow method
- Maximum clusters = min(10, papers_per_interval / 3)
- Minimum clusters = 2
- This ensures we don't over-cluster small intervals

**Example**:
- 30 papers → max 10 clusters
- 15 papers → max 5 clusters
- 6 papers → max 2 clusters

---

### Step 4: K-Means Clustering (Lines 157-159)

**Algorithm**: K-Means clustering on embedding vectors

```python
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(embeddings)
```

**What happens**:
- Papers are grouped into `optimal_k` clusters
- Each cluster represents a research topic
- Papers in the same cluster are semantically similar

**Visual Example**:
```
Papers: [P1, P2, P3, P4, P5, P6, P7, P8, P9, P10]
         ↓ K-Means Clustering ↓
Clusters: [0, 0, 1, 1, 1, 2, 2, 0, 1, 2]
          ↓
Topics:   Topic A (P1, P2, P8)
          Topic B (P3, P4, P5, P9)
          Topic C (P6, P7, P10)
```

---

### Step 5: Topic Coherence Calculation (Lines 168-176)

**Definition**: How similar are papers within the same topic cluster?

**Method**: Average cosine similarity within each cluster

```python
# For each cluster:
1. Get all embeddings in the cluster
2. Calculate pairwise cosine similarities
3. Remove diagonal (self-similarity = 1.0)
4. Average the similarities
```

**Formula**:
```
Coherence(cluster) = mean(cosine_similarity(paper_i, paper_j)) 
                     for all i, j in cluster where i ≠ j
```

**Interpretation**:
- **High coherence (0.8-1.0)**: Papers are very similar → well-defined topic
- **Low coherence (<0.5)**: Papers are diverse → loose topic or poor clustering

**Example**:
- Cluster A: 5 papers about "M&A integration" → coherence = 0.85 (high)
- Cluster B: 5 papers about "various strategy topics" → coherence = 0.45 (low)

---

### Step 6: Topic Diversity Calculation (Lines 195-202)

**Definition**: How evenly distributed are papers across topics?

**Method**: Normalized Shannon Entropy

```python
# Calculate proportions
proportions = [cluster_size / total_papers for each cluster]

# Calculate entropy
diversity = -sum(p * log(p)) / log(num_clusters)
```

**Formula**:
```
Diversity = -Σ(p_i * log(p_i)) / log(k)
where:
  p_i = proportion of papers in cluster i
  k = number of clusters
```

**Interpretation**:
- **1.0**: Perfect diversity (papers evenly distributed across all topics)
- **0.0**: No diversity (all papers in one topic)
- **0.5-0.8**: Moderate diversity

**Example**:
- 10 papers, 5 topics, each with 2 papers → diversity = 1.0 (perfect)
- 10 papers, 5 topics, one topic has 8 papers → diversity = 0.3 (low)

---

### Step 7: Representative Paper Identification (Lines 178-182)

For each topic cluster, find the paper closest to the cluster centroid:

```python
centroid = kmeans.cluster_centers_[cluster_id]
distances = euclidean_distance(paper_embeddings, centroid)
representative_paper = paper with minimum distance
```

**Purpose**: Identify the "most typical" paper for each topic

---

### Step 8: Cross-Interval Analysis - Stability (Lines 220-270)

**Definition**: How similar are topics across consecutive intervals?

**Method**: Compare cluster centroids between intervals

```python
# For interval i and i-1:
1. Get centroids from previous interval
2. Get centroids from current interval
3. Calculate cosine similarity matrix
4. Find best matching centroids
5. Average similarity = stability
```

**Formula**:
```
Stability(interval_i) = mean(max(cosine_similarity(centroid_prev, centroid_curr)))
```

**Interpretation**:
- **High stability (>0.7)**: Topics persist over time
- **Low stability (<0.4)**: Topics change rapidly

**Example**:
- Interval 1: Topic "M&A" (centroid A)
- Interval 2: Topic "M&A" (centroid B)
- Similarity(A, B) = 0.85 → High stability

---

### Step 9: Emerging Topics Detection (Lines 272-324)

**Definition**: New topics that didn't exist in the previous interval

**Method**: Compare centroids with similarity threshold

```python
# For each current interval:
1. Calculate similarity between current and previous centroids
2. Threshold = 0.7 (70% similarity)
3. Topics with similarity < 0.7 = Emerging
4. Previous topics not matched = Declining
```

**Algorithm**:
```
For each current_topic:
    max_similarity = max(cosine_similarity(current_topic, prev_topic) 
                        for all prev_topics)
    if max_similarity < 0.7:
        current_topic is EMERGING

For each previous_topic:
    max_similarity = max(cosine_similarity(prev_topic, current_topic) 
                        for all current_topics)
    if max_similarity < 0.7:
        previous_topic is DECLINING
```

**Example**:
- Previous interval: Topics A, B, C
- Current interval: Topics A, B, D
- Topic D (similarity to A/B/C < 0.7) → **Emerging**
- Topic C (not found in current) → **Declining**

---

## Summary Metrics

### Per Interval:
- **Topic Count**: Number of clusters identified
- **Coherence**: Average similarity within topics (0-1)
- **Diversity**: Entropy-based distribution measure (0-1)
- **Stability**: Similarity to previous interval topics (0-1)
- **Emerging Topics**: Count of new topics
- **Declining Topics**: Count of disappearing topics

### Overall Summary:
- **Average Topics per Interval**: Mean topic count
- **Average Coherence**: Mean coherence across intervals
- **Average Diversity**: Mean diversity across intervals

---

## Technical Details

### Embedding Model
- **Model**: `all-MiniLM-L6-v2` (SentenceTransformer)
- **Dimensions**: 384
- **Input**: Paper title + abstract
- **Output**: Dense vector representation

### Clustering Algorithm
- **Algorithm**: K-Means
- **Distance Metric**: Euclidean distance in embedding space
- **Initialization**: Random (10 initializations, best selected)
- **Convergence**: Standard K-Means convergence criteria

### Similarity Metrics
- **Cosine Similarity**: For coherence and stability
- **Euclidean Distance**: For representative paper selection
- **Shannon Entropy**: For diversity calculation

---

## Limitations & Considerations

1. **Minimum Papers Required**: Need at least 3 papers per interval for clustering
2. **Embedding Quality**: Depends on quality of paper abstracts/titles
3. **Cluster Count**: Simplified elbow method (could be improved with full elbow analysis)
4. **Threshold Sensitivity**: Emerging/declining topics depend on 0.7 similarity threshold
5. **Temporal Granularity**: 5-year intervals may miss shorter-term trends

---

## Future Improvements

1. **Dynamic Cluster Count**: Use full elbow method or silhouette analysis
2. **Hierarchical Clustering**: Identify sub-topics within main topics
3. **Topic Naming**: Use LLM to generate descriptive names for clusters
4. **Temporal Smoothing**: Account for gradual topic evolution
5. **Multi-Interval Tracking**: Track topics across multiple intervals (not just adjacent)

---

## Example Output

```json
{
  "interval": "2010-2014",
  "topic_count": 5,
  "coherence": 0.72,
  "diversity": 0.68,
  "stability": 0.65,
  "emerging_topics": [
    {"cluster_id": 3, "paper_count": 8}
  ],
  "declining_topics": [
    {"cluster_id": 2, "paper_count": 5}
  ],
  "topics": [
    {
      "cluster_id": 0,
      "paper_count": 12,
      "coherence": 0.78,
      "representative_paper": {
        "paper_id": "2012_123",
        "title": "Strategic Alliances in Global Markets"
      }
    },
    ...
  ]
}
```

---

## References

- **K-Means Clustering**: MacQueen, J. (1967). "Some Methods for classification and Analysis of Multivariate Observations"
- **Shannon Entropy**: Shannon, C. E. (1948). "A Mathematical Theory of Communication"
- **Sentence Transformers**: Reimers, N., & Gurevych, I. (2019). "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
